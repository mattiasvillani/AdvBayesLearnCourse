# Advanced Bayesian Learning - PhD course

---

### Course information

**The typical participant** is a PhD student in Statistics or related fields (Mathematical Statistics, Engineering Science, Quantitative Finance, Computer Science, ...). The participants are expected to have taken a basic course in Bayesian methods, for example Bayesian Learning at Linköping University or Bayesian Statistics I at Stockholm University.

**Examination and Grades**: The course is graded Pass or Fail. Examination is through individual reports on distributed problems for each topic. Many of the problems will require computer implementations of Bayesian learning algorithms.

**Course organization**
The course is organized in four topics, each containing four lecture hours. Course participants will spend most of their study time by solving the problem sets for each topic on their own computers without supervision.

All lectures are given online using Zoom this year.

Welcome!

[**Mattias Villani**](https://www.mattiasvillani.com/)  
Professor of Statistics, Stockholm and Linköping University

---


### Topic 1 - Gaussian Processes Regression and Classification

Reading:  [Gaussian Processes for Machine Learning](http://www.gaussianprocess.org/gpml/chapters/RW.pdf) - Chapters 1, 2.1-2.5, 3.1-3.4, 3.7, 4.1-4.3. \
Code: [GPML for Matlab](http://mlg.eng.cam.ac.uk/carl/gpml/) | [GPy for Python](https://sheffieldml.github.io/GPy/) | [Gausspr in R](https://rdrr.io/cran/kernlab/man/gausspr.html) | [Gaussianprocesses.jl in Julia](https://github.com/STOR-i/GaussianProcesses.jl) | [GPyTorch - GPs in PyTorch](https://gpytorch.ai/) \
Other material: [Visualize GP kernels](http://www.it.uu.se/edu/course/homepage/apml/GP/)


**Lecture 1 - April 17, hours 10-12**  
[slides](/Slides/ABL1.pdf)  
**Lecture 2 - April 17, hours 13-15**  
[slides](/Slides/ABL2.pdf)

**Lab Topic 1**  
[Problems](/Labs/Lab1.pdf) | [Lidar data](/Labs/LidarData.dat)


---


### Topic 2 - Bayesian Nonparametrics

Reading: [Bayesian Data Analysis](http://www.stat.columbia.edu/~gelman/book/BDA3.pdf) - Chapter 23 | [The Neal (2000) article on MCMC for Dirichlet Process Mixtures](http://www.stat.columbia.edu/npbayes/papers/neal_sampling.pdf)  


**Lecture 3 - April 28, hours 10-12**  
[slides](/Slides/ABL3.pdf)  
**Lecture 4 - April 28, hours 13-15**  
[slides](/Slides/ABL4.pdf) | [derivation marginal Gibbs](/Notes/MarginalGibbsDerivation.pdf)

**Lab Topic 2**  
[Problems](/Labs/Lab2.pdf) | [Galaxy data](/Labs/GalaxyData.dat)


---


### Topic 3 -  Variational Inference

Reading: [Blei et al JASA](https://amstat.tandfonline.com/doi/abs/10.1080/01621459.2017.1285773#.XraDPXUzaLI) | [Tran's VI Notes](/Material/VBnotesMNT.pdf) \
Other material: [Natural gradient notes](https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/) | [autograd in python](https://github.com/HIPS/autograd) | [ForwardDiff in Julia](https://github.com/JuliaDiff/ForwardDiff.jl)

**Lecture 5 - May 15, hours 10-12**  
[slides](/Slides/ABL5.pdf)  
**Lecture 6 - May 15, hours 13-15**  
[slides](/Slides/ABL6.pdf)

**Lab Topic 3**  
[Problems](/Labs/Lab3.pdf) | [Time series data](/Labs/timeseries.csv)


---


### Topic 4 - Bayesian Model Inference

Reading (ordered by priority): [Bayesian Data Analysis](http://www.stat.columbia.edu/~gelman/book/BDA3.pdf) - Chapter 7
| [Bayesian predictive methods article](https://link.springer.com/article/10.1007/s11222-016-9649-y) | [LOO-CV and WAIC article](https://link.springer.com/article/10.1007/s11222-016-9696-4) | [Bayesian regularization and Horseshoe](https://onlinelibrary-wiley-com.ezp.sub.su.se/doi/full/10.1002/wics.1463) | [Gaussian Processes for Machine Learning](http://www.gaussianprocess.org/gpml/chapters/RW.pdf) - Chapters 5.1-5.4  

**Lecture 7 - May 29, hours 10-12**  
[slides](/Slides/ABL7.pdf)  
**Lecture 8 - May 29, hours 13-15**  
[slides](/Slides/ABL8.pdf)

**Lab Topic 4**  
[Problems](TBA)
